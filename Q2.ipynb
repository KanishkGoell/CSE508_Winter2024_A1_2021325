{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kanishk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kanishk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data (if not already done)\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "\n",
    "# Define the directory containing your text files\n",
    "directory = \"text_files\"\n",
    "preprocessed_directory = \"preprocessed_files\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize the text into words.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from the list of tokens.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    \"\"\"Remove punctuation from the list of tokens.\"\"\"\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "def remove_blank_tokens(tokens):\n",
    "    \"\"\"Remove any blank space tokens.\"\"\"\n",
    "    return [token for token in tokens if token.strip() != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Process and Save Preprocessed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text in file502.txt:\n",
      "Kit is awesome. I play in my garage just for personal enjoyment not for performances or anything. Once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. With the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "After a few weeks of daily use for at least an hour a day it still looks and plays beautifully. Overall one of the best purchases I could have made.\n",
      "\n",
      "Preprocessed text in file502.txt:\n",
      "kit awesome play garage personal enjoyment performances anything take time break settings able dial pretty much kit sound expansion options relatively inexpensive parts expanding easy fun weeks daily use least hour day still looks plays beautifully overall one best purchases could made\n",
      "\n",
      "----------------------------------------\n",
      "Original text in file264.txt:\n",
      "I just tested this fog fluid with a 1byone 400W fogger. Two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. This being a hot space I was pleasantly surprised by how long the fog would linger. It would quickly rise to eye level and then just hang there. Another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "Only downside is that the fog is not very dense. The difference between 2 bursts and 5 bursts was not too pronounced; it's a grey mist that does not become white or truly opaque, with visibility remaining at > 9 feet.\n",
      "\n",
      "Preprocessed text in file264.txt:\n",
      "tested fog fluid fogger two second bursts sufficient create enough fog layers moody atmosphere car garage hot space pleasantly surprised long fog would linger would quickly rise eye level hang another nice surprise much step middle thick pocket smells like lavender soap downside fog dense difference bursts bursts pronounced grey mist become white truly opaque visibility remaining feet\n",
      "\n",
      "----------------------------------------\n",
      "Original text in file270.txt:\n",
      "Do not let the low price fool you! This is an incredible device with the free mixing software. It has been years since i tracked anything. Back then everything was Solid State and Analogue. With this Scarlett Solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "So if you are new or getting back to it, GET IT!\n",
      "\n",
      "Pro's\n",
      "Customer Support\n",
      "You Tube videos\n",
      "\n",
      "Con's\n",
      "TOO many features with the Scarlet Pro Software.\n",
      "A instructional, \"downloadable\" simple user manual.\n",
      "\n",
      "Preprocessed text in file270.txt:\n",
      "let low price fool incredible device free mixing software years since tracked anything back everything solid state analogue scarlett solo moderate computer large monitor stress large much control software really lay serious tracks new getting back get customer support tube videos many features scarlet pro software instructional downloadable simple user manual\n",
      "\n",
      "----------------------------------------\n",
      "Original text in file516.txt:\n",
      "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
      "\n",
      "Preprocessed text in file516.txt:\n",
      "using several wall built hangers work great several different shapes wish available bands straps locking devices keep instruments secure earthquake prone california ended using velcro strips hangers affix fretboard otherwise great hanger\n",
      "\n",
      "----------------------------------------\n",
      "Original text in file258.txt:\n",
      "Poor design doesn't line their own pedals up when connected using these. The offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. As you can see in the picture, the third pedal from the chain is already almost off the board. How can they overlook this?\n",
      "\n",
      "Preprocessed text in file258.txt:\n",
      "poor design line pedals connected using offset enough results pedal along line mounted bit lower one next see picture third pedal chain already almost board overlook\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, process only the first 5 files\n",
    "sample_files = os.listdir(directory)[:999]\n",
    "count = 0\n",
    "\n",
    "for filename in sample_files:\n",
    "    original_filepath = os.path.join(directory, filename)\n",
    "    count = count + 1\n",
    "    with open(original_filepath, 'r', encoding='utf-8') as file:\n",
    "        \n",
    "        original_text = file.read()\n",
    "        if(count <= 5):\n",
    "            print(f\"Original text in {filename}:\\n{original_text}\\n\")\n",
    "        \n",
    "        # a. Lowercase the text\n",
    "        text_lower = to_lowercase(original_text)\n",
    "        \n",
    "        # b. Perform tokenization\n",
    "        tokens = tokenize_text(text_lower)\n",
    "        \n",
    "        # c. Remove stopwords\n",
    "        tokens_no_stopwords = remove_stopwords(tokens)\n",
    "        \n",
    "        # d. Remove punctuations\n",
    "        tokens_no_punctuation = remove_punctuation(tokens_no_stopwords)\n",
    "        \n",
    "        # e. Remove blank space tokens\n",
    "        final_tokens = remove_blank_tokens(tokens_no_punctuation)\n",
    "        preprocessed_text = ' '.join(final_tokens)\n",
    "        \n",
    "        # Print preprocessed text for comparison\n",
    "        if(count <= 5):\n",
    "            print(f\"Preprocessed text in {filename}:\\n{preprocessed_text}\\n\")\n",
    "            print (\"-\" * 40)\n",
    "        \n",
    "        # Define the path for the preprocessed file\n",
    "        preprocessed_filepath = os.path.join(preprocessed_directory, f\"preprocessed_{filename}\")\n",
    "        \n",
    "        # Save the preprocessed text to the new file in preprocessed_files folder\n",
    "        with open(preprocessed_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(preprocessed_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
