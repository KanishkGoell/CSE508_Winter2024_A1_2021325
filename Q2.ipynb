{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a Unigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assuming 'directory' contains the preprocessed files\n",
    "preprocessed_directory = \"preprocessed_files\"\n",
    "\n",
    "# Initialize an empty inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# Populate the inverted index\n",
    "for filename in os.listdir(preprocessed_directory):\n",
    "    filepath = os.path.join(preprocessed_directory, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for word in file.read().split():\n",
    "            if word in inverted_index:\n",
    "                inverted_index[word].add(filename)\n",
    "            else:\n",
    "                inverted_index[word] = {filename}\n",
    "\n",
    "# Save the inverted index using pickle\n",
    "with open('inverted_index.pkl', 'wb') as f:\n",
    "    pickle.dump(inverted_index, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inverted index\n",
    "with open('inverted_index.pkl', 'rb') as f:\n",
    "    loaded_inverted_index = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Boolean Query Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_and(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "def perform_or(set1, set2):\n",
    "    return set1.union(set2)\n",
    "\n",
    "def perform_and_not(set1, set2):\n",
    "    return set1 - set2\n",
    "\n",
    "def perform_or_not(set1, set2, all_docs):\n",
    "    return set1.union(all_docs - set2)\n",
    "\n",
    "# Helper function to get all document names\n",
    "def get_all_document_names(directory):\n",
    "    return set(os.listdir(directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Process Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: guitar AND sounds\n",
      "Number of documents retrieved for query 1: 27\n",
      "Names of the documents retrieved for query 1: preprocessed_file514.txt, preprocessed_file804.txt, preprocessed_file277.txt, preprocessed_file166.txt, preprocessed_file183.txt, preprocessed_file407.txt, preprocessed_file571.txt, preprocessed_file729.txt, preprocessed_file593.txt, preprocessed_file569.txt, preprocessed_file541.txt, preprocessed_file982.txt, preprocessed_file413.txt, preprocessed_file396.txt, preprocessed_file936.txt, preprocessed_file801.txt, preprocessed_file978.txt, preprocessed_file484.txt, preprocessed_file324.txt, preprocessed_file840.txt, preprocessed_file903.txt, preprocessed_file361.txt, preprocessed_file470.txt, preprocessed_file271.txt, preprocessed_file106.txt, preprocessed_file98.txt, preprocessed_file405.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_query(query, operations, all_docs, inverted_index):\n",
    "    # Preprocess and tokenize the query\n",
    "    preprocessed_query = query.lower()  # Assuming simple lowercase conversion for demo\n",
    "    tokens = preprocessed_query.split()  # This would ideally use the same preprocessing as your documents\n",
    "    # Create a formatted query for output\n",
    "    formatted_query = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        formatted_query += token\n",
    "        if i < len(operations):\n",
    "            formatted_query += \" \" + operations[i] + \" \"\n",
    "    \n",
    "    # Convert tokens to their document sets\n",
    "    query_sets = [inverted_index.get(token, set()) for token in tokens]\n",
    "    \n",
    "    # Initialize the result set based on the first token's document set\n",
    "    result_set = query_sets[0] if query_sets else set()\n",
    "    \n",
    "    # Apply operations with subsequent tokens\n",
    "    for op, next_set in zip(operations, query_sets[1:]):\n",
    "        if op.strip() == \"AND\":\n",
    "            result_set = perform_and(result_set, next_set)\n",
    "        elif op.strip() == \"OR\":\n",
    "            result_set = perform_or(result_set, next_set)\n",
    "        elif op.strip() == \"AND NOT\":\n",
    "            result_set = perform_and_not(result_set, next_set)\n",
    "        elif op.strip() == \"OR NOT\":\n",
    "            result_set = perform_or_not(result_set, next_set, all_docs)\n",
    "            \n",
    "    return formatted_query, result_set\n",
    "\n",
    "# Example static input structure for demonstration\n",
    "N = int(input(\"Enter the number of queries: \"))\n",
    "queries = []\n",
    "operations_list = []\n",
    "\n",
    "for i in range(N):\n",
    "    query = input(f\"Enter query {i+1}: \")\n",
    "    operations = input(\"Enter operations separated by commas (e.g., AND,OR NOT): \").split(', ')\n",
    "    queries.append(query)\n",
    "    operations_list.append(operations)\n",
    "\n",
    "all_docs = set(get_all_document_names(preprocessed_directory))\n",
    "\n",
    "for i, (query, operations) in enumerate(zip(queries, operations_list), start=1):\n",
    "    formatted_query, result_docs = process_query(query, operations, all_docs, loaded_inverted_index)\n",
    "    print(f\"Query {i}: {formatted_query}\")\n",
    "    print(f\"Number of documents retrieved for query {i}: {len(set(result_docs))}\")\n",
    "    if result_docs:\n",
    "        print(f\"Names of the documents retrieved for query {i}: {', '.join(result_docs)}\\n\")\n",
    "    else:\n",
    "        print(\"No documents retrieved for this query.\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
